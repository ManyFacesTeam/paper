<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
    <meta charset="utf-8">
    <meta name="generator" content="quarto-1.6.42">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


    <title>Capturing Many Faces</title>
    <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.columns{display: flex; gap: min(4vw, 1.5em);}
      div.column{flex: auto; overflow-x: auto;}
      div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
      ul.task-list{list-style: none;}
      ul.task-list li input[type="checkbox"] {
        width: 0.8em;
        margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
        vertical-align: middle;
      }
      /* CSS for syntax highlighting */
      pre > code.sourceCode { white-space: pre; position: relative; }
      pre > code.sourceCode > span { line-height: 1.25; }
      pre > code.sourceCode > span:empty { height: 1.2em; }
      .sourceCode { overflow: visible; }
      code.sourceCode > span { color: inherit; text-decoration: inherit; }
      div.sourceCode { margin: 1em 0; }
      pre.sourceCode { margin: 0; }
      @media screen {
      div.sourceCode { overflow: auto; }
      }
      @media print {
      pre > code.sourceCode { white-space: pre-wrap; }
      pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
      }
      pre.numberSource code
        { counter-reset: source-line 0; }
      pre.numberSource code > span
        { position: relative; left: -4em; counter-increment: source-line; }
      pre.numberSource code > span > a:first-child::before
        { content: counter(source-line);
          position: relative; left: -1em; text-align: right; vertical-align: baseline;
          border: none; display: inline-block;
          -webkit-touch-callout: none; -webkit-user-select: none;
          -khtml-user-select: none; -moz-user-select: none;
          -ms-user-select: none; user-select: none;
          padding: 0 4px; width: 4em;
        }
      pre.numberSource { margin-left: 3em;  padding-left: 4px; }
      div.sourceCode
        {   }
      @media screen {
      pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
      }
    </style>

    <style>
      body.hypothesis-enabled #quarto-embed-header {
        padding-right: 36px;
      }

      #quarto-embed-header {
        height: 3em;
        width: 100%;
        display: flex;
        justify-content: space-between;
        align-items: center;
        border-bottom: solid 1px;
      }

      #quarto-embed-header h6 {
        font-size: 1.1em;
        padding-top: 0.6em;
        margin-left: 1em;
        margin-right: 1em;
        font-weight: 400;
      }

      #quarto-embed-header a.quarto-back-link,
      #quarto-embed-header a.quarto-download-embed {
        font-size: 0.8em;
        margin-top: 1em;
        margin-bottom: 1em;
        margin-left: 1em;
        margin-right: 1em;
      }

      .quarto-back-container {
        padding-left: 0.5em;
        display: flex;
      }

      .headroom {
          will-change: transform;
          transition: transform 200ms linear;
      }

      .headroom--pinned {
          transform: translateY(0%);
      }

      .headroom--unpinned {
          transform: translateY(-100%);
      }      
    </style>

    <script>
    window.document.addEventListener("DOMContentLoaded", function () {

      var header = window.document.querySelector("#quarto-embed-header");
      const titleBannerEl = window.document.querySelector("body > #title-block-header");
      if (titleBannerEl) {
        titleBannerEl.style.paddingTop = header.clientHeight + "px";
      }
      const contentEl = window.document.getElementById('quarto-content');
      for (const child of contentEl.children) {
        child.style.paddingTop = header.clientHeight + "px";
        child.style.marginTop = "1em";
      }

      // Use the article root if the `back` call doesn't work. This isn't perfect
      // but should typically work
      window.quartoBackToArticle = () => {
        var currentUrl = window.location.href;
        window.history.back();
        setTimeout(() => {
            // if location was not changed in 100 ms, then there is no history back
            if(currentUrl === window.location.href){              
                // redirect to site root
                window.location.href = "index.html";
            }
        }, 100);
      }

      const headroom = new window.Headroom(header, {
        tolerance: 5,
        onPin: function () {
        },
        onUnpin: function () {
        },
      });
      headroom.init();
    });
    </script>

    
<script src="site_libs/manuscript-notebook/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-10daa034703793678e481cc8cee6d76f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
     <script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>  <link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>  
      <meta name="citation_title" content="Capturing Many Faces">
<meta name="citation_author" content="Thora Bjornsdottir">
<meta name="citation_author" content="Vít Třebický">
<meta name="citation_author" content="Lisa DeBruine">
<meta name="citation_language" content="en">
</head>

  <body class="quarto-notebook">
    <div id="quarto-embed-header" class="headroom fixed-top bg-primary">
      
      <a onclick="window.quartoBackToArticle(); return false;" class="btn btn-primary quarto-back-link" href=""><i class="bi bi-caret-left"></i> Back to Article</a>
      <h6><i class="bi bi-journal-code"></i> Article Notebook</h6>

            <a href="./index.qmd" class="btn btn-primary quarto-download-embed" download="index.qmd">Download Source</a>
          </div>

     <header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Capturing Many Faces</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Authors</div>
          <div class="quarto-title-meta-heading">Affiliations</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Thora Bjornsdottir <a href="mailto:thora.bjornsdottir@stir.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-1016-3829" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        University of Stirling
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Vít Třebický <a href="mailto:vit.trebicky@natur.cuni.cz" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-1440-1772" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Charles University
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Lisa DeBruine <a href="mailto:lisa.debruine@glasgow.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-7523-5539" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        University of Glasgow
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      </div>
    </div>



    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#existing-face-databases" id="toc-existing-face-databases" class="nav-link" data-scroll-target="#existing-face-databases"><span class="header-section-number">1.1</span> Existing face databases</a></li>
  <li><a href="#the-current-work" id="toc-the-current-work" class="nav-link" data-scroll-target="#the-current-work"><span class="header-section-number">1.2</span> The current work</a></li>
  </ul></li>
  <li><a href="#studyphase-1-pprotocol-development-stimulus-collection" id="toc-studyphase-1-pprotocol-development-stimulus-collection" class="nav-link" data-scroll-target="#studyphase-1-pprotocol-development-stimulus-collection"><span class="header-section-number">2</span> Study/phase 1: Pprotocol development &amp; stimulus collection</a>
  <ul class="collapse">
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method"><span class="header-section-number">2.1</span> Method</a></li>
  <li><a href="#stimulus-collection" id="toc-stimulus-collection" class="nav-link" data-scroll-target="#stimulus-collection"><span class="header-section-number">2.2</span> Stimulus Collection</a></li>
  </ul></li>
  <li><a href="#studyphase-2-validationnorming-data" id="toc-studyphase-2-validationnorming-data" class="nav-link" data-scroll-target="#studyphase-2-validationnorming-data"><span class="header-section-number">3</span> Study/phase 2: Validation/norming data</a>
  <ul class="collapse">
  <li><a href="#method-1" id="toc-method-1" class="nav-link" data-scroll-target="#method-1"><span class="header-section-number">3.1</span> Method</a></li>
  <li><a href="#measures" id="toc-measures" class="nav-link" data-scroll-target="#measures"><span class="header-section-number">3.2</span> Measures</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">3.3</span> Results</a></li>
  <li><a href="#agreement-indicators-for-the-ratings" id="toc-agreement-indicators-for-the-ratings" class="nav-link" data-scroll-target="#agreement-indicators-for-the-ratings"><span class="header-section-number">3.4</span> Agreement Indicators for the Ratings</a></li>
  <li><a href="#descriptive-statistics" id="toc-descriptive-statistics" class="nav-link" data-scroll-target="#descriptive-statistics"><span class="header-section-number">3.5</span> Descriptive Statistics</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">4</span> Discussion</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">      

       <div class="cell-container"><div class="cell-decorator"><pre>In [1]:</pre></div><div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.2
✔ ggplot2   4.0.0     ✔ tibble    3.3.0
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.1.0     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
</div></div>
<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Centrality of the face in human perception &amp; in research Variety of social perception &amp; recognition topics that use face stimuli Importance of open &amp; transparent methods, current limits for this in terms of stimuli currently</p>
<section id="existing-face-databases" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="existing-face-databases"><span class="header-section-number">1.1</span> Existing face databases</h3>
<ul>
<li>A wealth of face stimulus databases exist, created for varying purposes. Many of these are available for research use (see e.g., face image meta-database to search many of these) Many studies use the same stimulus databases (e.g., CFD, AFD, Karolinska – check citations of the big ones)</li>
<li>This can cause issues in terms of generalisability</li>
<li>Participants online may become familiar with frequently used stimuli</li>
<li>Other studies use stimuli they collect themselves but cannot necessarily share for ethical reasons</li>
<li>These are often not well-documented, creates an issue for transparency &amp; replication attempts (can cite Grim Image preprint)</li>
<li>There are also limitations in terms of many databases’ diversity of stimuli (e.g., many include only White or Western targets/models)</li>
<li>Recent databases are improving on this (e.g., Hawaii face database, mixed-race database – check Zotero for recent saved papers)</li>
<li>Crucially, most databases are static. Once collected, the number of stimuli available does not change. (exception: CFD)</li>
</ul>
</section>
<section id="the-current-work" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="the-current-work"><span class="header-section-number">1.2</span> The current work</h3>
<ul>
<li>Sought to address the limitations of existing databases</li>
<li>Collected face images in the lab for experimental control &amp; to have associated demographic data + from consenting individuals</li>
<li>Vs scraping images from online</li>
<li>Developed a reproducible and openly-available protocol for face image collection, collecting multiple images per target/model to benefit a wide array of possible research areas/questions</li>
<li>Transparently documents the process of image collection</li>
<li>Enables reproducibility</li>
<li>The database can be added to in the future (i.e., it will not be static)</li>
<li>Collected images following the protocol in 20 different labs across the world</li>
<li>specify # of countries, world regions (compare to existing databases?)</li>
<li>Diverse set of models (more so than possible to collect at just one lab)</li>
<li>Recruited online perceivers to rate a subset of the images (i.e., front-facing images) to generate norming data</li>
<li>Make the database available for future research</li>
<li>Altogether, we introduce a new, diverse, openly-available face stimulus set that will continue to grow in the future. We hope that this will broaden the generalisability of findings in face perception and recognition research.</li>
</ul>
</section>
</section>
<section id="studyphase-1-pprotocol-development-stimulus-collection" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="studyphase-1-pprotocol-development-stimulus-collection"><span class="header-section-number">2</span> Study/phase 1: Pprotocol development &amp; stimulus collection</h2>
<section id="method" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="method"><span class="header-section-number">2.1</span> Method</h3>
<p>For both studies/phases, the University of Glasgow provided ethical approval. We obtained additional local ethical approval at collaborating institutions where required.</p>
<section id="protocol-development" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="protocol-development"><span class="header-section-number">2.1.1</span> Protocol Development</h4>
<p>The team developed and pre-tested a protocol for collecting standardised and reproducible images of faces for use in research. Following a survey among the ManyFaces team, we constructed the protocol for collection of a variety of images that would be useful for a broad variety of research questions. Specifically, we included images varying in their standardisation of appearance (standardised and unstandardised, e.g., white t-shirt and hair pulled back vs.&nbsp;clothing and hair as worn by the participant/model on that day), viewing angle (frontal, profile, ¾), and facialemotion expression (neutral, natural, angry, disgusted, fearful, happy, sad, surprised).</p>
<p>Prior to beginning data collection, the team pre-tested the protocol to ensure clarity of instructions and consistency of images collected across sites. We revised details within the protocol to resolve issues that arose (e.g., revised facial expression emotion elicitation instructions, clarified camera settings). The final protocol can be found at [https://docs.google.com/document/d/1D9TPGXCgTRZi7nqEIg6jb42R4gNQx9L3qasg8tFvu1I/edit?usp=sharing]. In sum, the protocol detailed the following:</p>
<ul>
<li>Image Types. The protocol defined the categories of images in terms of their standardisation of appearance, viewing angle, and facial expression.</li>
<li>Standardisation of Appearance. In the standardised images models wore a white t-shirt, had their hair pulled back or covered, included no adornments/accessories (excepting those that could not be removed for cultural reasons), and wore minimal or no makeup. In contrast, unstandardised images showed models in their own clothing, with their hair as they came into the lab, and with adornments excepting glasses or anything obscuring the face or neck.</li>
<li>Viewing Angle. Full frontal portraits showed models facing the camera, left and right profile portraits showed each side of models’ faces in profile, and left and right ¾ portraits showed models’ faces at a 45-degree angle from the axis of the camera.</li>
<li>Facial Expression. Neutral images were of models refraining from making any facial expression, natural images displayed a natural expression for the model, and each facial expression of emotion displayed the expression the model would make if they were feeling the specified emotion (e.g., happiness).</li>
</ul>
</section>
<section id="image-prioritisation" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="image-prioritisation"><span class="header-section-number">2.1.2</span> Image Prioritisation</h4>
<p>The protocol specified which images to take (i.e., the combinations of appearance standardisation, viewing angle, and emotion expression), in what order, and which were most crucial to collect if researchers were under time constraints. The order of image collection was as follows, with starred (*) images prioritised/required for each model:</p>
<ul>
<li>Exposure calibration photo*</li>
<li>Identification &amp; calibration photo*</li>
<li>Unstandardized - Natural - Full frontal portrait</li>
<li>Unstandardized - Neutral - Full frontal portrait*</li>
<li>Unstandardized - Neutral - Left and Right profile portraits</li>
<li>Unstandardized - Neutral - Left and Right ¾ portraits</li>
<li>Unstandardized - Happy - Full frontal portrait*</li>
<li>Unstandardized - Happy - Left and Right profile portraits</li>
<li>Unstandardized - Happy - Left and Right ¾ portraits</li>
<li>Unstandardized - All other expressions (in random or appropriate order) - Full frontal portrait</li>
<li>Standardized - Natural - Full frontal portrait</li>
<li>Standardized - Neutral - Full frontal portrait*</li>
<li>Standardized - Neutral - Left and Right profile portraits*</li>
<li>Standardized - Neutral - Left and Right ¾ portraits*</li>
<li>Standardized - Happy - Full frontal portrait*</li>
<li>Standardized - Happy - Left and Right profile portraits</li>
<li>Standardized - Happy - Left and Right ¾ portraits</li>
<li>Standardized - All other expressions (in random or appropriate order) - Full frontal portrait</li>
</ul>
<p>Researchers with additional time could also capture the other expressions at different viewing angles (Left/Right profile, ¾) with either unstandardized or standardized appearance, as well as a video of standardized appearance to test out 3D image capture.</p>
</section>
<section id="equipment" class="level4" data-number="2.1.3">
<h4 data-number="2.1.3" class="anchored" data-anchor-id="equipment"><span class="header-section-number">2.1.3</span> Equipment</h4>
<p>All collaborating sites/labs used the same set of equipment. Images were captured using a Canon EOS 250d (also called Canon EOS Rebel SL3 in some regions) camera with Canon 18-55mm IS STM lens, and lit by an LED Ring light (Fovitec Bi-Colour LED 18” Ring Light) on a stand. Colour calibration was via a Calibrite colourChecker Classic card. The image collection space had a white background with a chair for models to sit in. Researchers provided white t-shirts for models to wear for standardised images.</p>
</section>
<section id="setup" class="level4" data-number="2.1.4">
<h4 data-number="2.1.4" class="anchored" data-anchor-id="setup"><span class="header-section-number">2.1.4</span> Setup</h4>
<p>The setup of the room used for image collection at each site required removing or minimising outside light sources (e.g., using a windowless room, turning off overhead lights) and colour spill. The protocol specified the distances at which to set up the chair for models to sit in relative to the white background and the lighting and camera rig. Camera setup instructions including the specified shooting mode, shutter speed, aperture, ISO, white balance, and colour space.</p>
</section>
<section id="procedure" class="level4" data-number="2.1.5">
<h4 data-number="2.1.5" class="anchored" data-anchor-id="procedure"><span class="header-section-number">2.1.5</span> Procedure</h4>
<p>Finally, the protocol detailed how to prepare and position models and how to take each kind of images listed in the image prioritisation section. This included the positioning of the model’s head for each viewing angle and facial expression elicitation instructions.</p>
</section>
</section>
<section id="stimulus-collection" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="stimulus-collection"><span class="header-section-number">2.2</span> Stimulus Collection</h3>
<p>Following the protocol, 20 labs (1 in Austria, 2 in Brazil, 2 in Canada, 2 in Germany, 1 in Israel, 3 in Malaysia, 1 in Mexico, 1 in the Netherlands, 1 in Serbia, 4 in the UK, and 2 in the US) [note # of labs per country] collected images of an average of 10 models per site. As specified by the protocol, researchers collected multiple images of each model, within the time constraints of the study session.</p>
<p>Models furthermore completed a demographic questionnaire via Experimentum, reporting their age, gender, ethnicity, height, and weight. They also reported whether or not they were wearing makeup (specifying what kinds, e.g., foundation, eye makeup, semi-permanent makeup) and whether they had ever experienced anything that could affect the shape of their face (e.g., broken nose, cosmetic surgery or injections, orthodontic work) and specified this if they were willing.… Finally, they completed a debriefing questionnaire asking them about their experience of posing for the images (e.g., whether instructions were clear, whether any part of the process was uncomfortable). Results</p>
<p>211 total models provided their images, following withdrawals and exclusions for poor image quality. <a href="#tbl-images-per-type" class="quarto-xref">Table&nbsp;1</a> details the number of models for Detail resulting number of models from each site [after exclusions for photo quality &amp; model withdrawals], number of each kind of photo. collected</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [2]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [3]:</pre></div><div><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-images-per-type" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl quarto-uncaptioned" id="tbl-images-per-type-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1
</figcaption>
<div aria-describedby="tbl-images-per-type-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[],"data":[],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<p>[link to where the images can be requested] [example images]</p>
<p>For data cleaning, we first downloaded and reshaped the raw data from Experimentum. In the next step, we ensured that the models’ gender, race/ethnicity, and units of height and weight were consistently formatted across labs. For gender and race/ethnicity, words presented in languages other than English were recoded to be presented in English (e.g., “mulher” to “female”, “preta” to “Black”). We then classified self-described race/ethnicity into one of seven categories (“White”, “Black”, “Asian”, “Indigenous”, “MENA” (Middle Eastern or North African), “Latine”, or “Mixed”), where possible. Descriptions that could not be clearly sorted into these categories were given “Ambiguous label” (non-entries were recoded as NA). For height, we ensured all data were presented in centimetres, and for weight, we ensured that all data were presented in kilograms. Models could report height and weight in metric or imperial units, so we converted from imperial to metric where required. We also sanity-checked the reported units, assuming heights &gt; 100 to be in centimetres and those &lt; 100 to be in inches, and weights &lt; 80 to be in kilograms. Any non-entries of height or weight for a model was recoded as NA.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [4]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [5]:</pre></div><div><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">a =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-model-demog" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-demog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Summary of model demographic info
</figcaption>
<div aria-describedby="tbl-model-demog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["a"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
</section>
</section>
<section id="studyphase-2-validationnorming-data" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="studyphase-2-validationnorming-data"><span class="header-section-number">3</span> Study/phase 2: Validation/norming data</h2>
<p>We next obtained perceptions/ratings of a subset of the photos (front-facing) to validate the emotion expressions (their perceived emotion category and intensity) and collect norming data on central social perceptions, namely perceived attractiveness, dominance, trustworthiness, gender-typicality, memorability, and age. We chose these ratings due to their central importance in the person perception, face recognition, and emotion perception literatures (e.g., Oosterhof &amp; Todorov, 2008; Sutherland et al., 2013; Perrett, 2010; –add emotion &amp; memorability/recognition cites–). We preregistered this study on the OSF (<a href="https://osf.io/4d5v9" class="uri">https://osf.io/4d5v9</a>).</p>
<section id="method-1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="method-1"><span class="header-section-number">3.1</span> Method</h3>
<section id="image-processing" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="image-processing"><span class="header-section-number">3.1.1</span> Image Processing</h4>
<p>RAW images were processed using webmorphR (https://doi.org/10.31234/osf.io/j2754), which facilitates scriptable processing of images using imagemagick; a full script of the processing steps is available at REPO LINK. Briefly,</p>
<ul>
<li>Each face was delineated using the Face++ automatic face detection algorithm to generate a 106-point template.</li>
<li>All images were resized to 1000w by 1500h pixels to standardise size (two different RAW formats were used, which resulted in two different image sizes)</li>
<li>The median RGB colour value of a 100x100 pixel patch at the upper left corner was calculated to fill in any edges from alignment.</li>
<li>The image was repositioned and cropped (not rotated or resized) such that
<ul>
<li>The image size was 675w by 900h</li>
<li>point 71 (between the eyes) was relocated to position [.5w, .4h]</li>
</ul></li>
<li>This aligned image was saved as a lossless PNG using imagemagick default settings (e.g., sRGB colour space).</li>
<li>A white balance correction was applied to the resulting images, calculated from the mean RGB values in the 25x25 pixel top-right corner patch</li>
<li>These images were converted to JPEGs with a quality setting of 75 to reduce file size for stimulus display online.</li>
</ul>
<div class="cell-container"><div class="cell-decorator"><pre>In [6]:</pre></div><div id="cell-fig-img-processing" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-img-processing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-img-processing-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Visualise some steps of this with a delineated original, aligned version, and white balanced version."><img src="index_files/figure-html/fig-img-processing-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Visualise some steps of this with a delineated original, aligned version, and white balanced version.
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
<section id="stimuli" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="stimuli"><span class="header-section-number">3.1.2</span> Stimuli</h4>
<p>The number of stimuli was determined by the number of modelstargets recruited across the 20 research labs and how many models posed for each image type; the maximum number of targets per image type was 205in total we have 205 targets, which means that each rater saw up to 205 stimuli. We obtained ratings of the front-facing standardised neutral (n = 205), unstandardised neutral (n = 188), standardised angry (n = 187), standardised disgusted (n = 184), standardised fearful (n = 175), standardised happy (n = 199), standardised sad (n = 183), and standardised surprised (n = 184).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [7]:</pre></div><div id="cell-fig-stim-examples" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-stim-examples" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stim-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-stim-examples-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: An example of each stimulus (could incorporate the N table)"><img src="index_files/figure-html/fig-stim-examples-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stim-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: An example of each stimulus (could incorporate the N table)
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
<section id="attention-check-stimuli" class="level4" data-number="3.1.3">
<h4 data-number="3.1.3" class="anchored" data-anchor-id="attention-check-stimuli"><span class="header-section-number">3.1.3</span> Attention Check Stimuli</h4>
<p>Additionally, we created stimuli for attention checks. These were white images with the same size and aspect ratio as the face stimuli, but contained only the written instruction to choose a specific response, (e.g., ‘Choose “fear”’ or ’Choose 3”).</p>
</section>
</section>
<section id="measures" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="measures"><span class="header-section-number">3.2</span> Measures</h3>
<section id="standardised-neutral-faces" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="standardised-neutral-faces"><span class="header-section-number">3.2.1</span> Standardised Neutral Faces</h4>
<section id="trait-ratings" class="level5" data-number="3.2.1.1">
<h5 data-number="3.2.1.1" class="anchored" data-anchor-id="trait-ratings"><span class="header-section-number">3.2.1.1</span> Trait Ratings</h5>
<p>We obtained ratings of faces’ attractiveness, dominance, trustworthiness, memorability, and gender-typicality (‘How attractive [dominant, trustworthy, memorable, gender-typical] does this person look?’). Ratings were on scales ranging from 1 (not at all) to 7 (very)</p>
</section>
<section id="demographic-impressions" class="level5" data-number="3.2.1.2">
<h5 data-number="3.2.1.2" class="anchored" data-anchor-id="demographic-impressions"><span class="header-section-number">3.2.1.2</span> Demographic Impressions</h5>
<p>We obtained ratings of faces’ perceived age (‘How old does this person look?’), with responses collected in 5-year ranges/brackets (i.e., 16-20, 21-25, …, 76-80, 81+).</p>
</section>
</section>
<section id="unstandardised-neutral-faces" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="unstandardised-neutral-faces"><span class="header-section-number">3.2.2</span> Unstandardised Neutral Faces</h4>
<section id="trait-ratings-1" class="level5" data-number="3.2.2.1">
<h5 data-number="3.2.2.1" class="anchored" data-anchor-id="trait-ratings-1"><span class="header-section-number">3.2.2.1</span> Trait Ratings</h5>
<p>We obtained ratings of faces’ attractiveness, dominance, and trustworthiness on scales ranging from 1 (not at all) to 7 (very)</p>
</section>
</section>
<section id="standardised-emotional-faces" class="level4" data-number="3.2.3">
<h4 data-number="3.2.3" class="anchored" data-anchor-id="standardised-emotional-faces"><span class="header-section-number">3.2.3</span> Standardised Emotional Faces</h4>
<section id="emotion-categorisation" class="level5" data-number="3.2.3.1">
<h5 data-number="3.2.3.1" class="anchored" data-anchor-id="emotion-categorisation"><span class="header-section-number">3.2.3.1</span> Emotion Categorisation</h5>
<p>We obtained impressions of the emotion each person was expressing (‘What emotion is this person expressing?’), choosing one from: anger, disgust, fear, happiness, sadness, surprise, other. Here, raters categorised a counterbalanced mixture of expressions (from one of six counterbalanced conditions) rather than faces all showing the same expression. The 201 identities with emotion images were divided into six groups of up to 34 images, and each counterbalanced condition showed a different emotion for each of the six groups, such that no identity was shown more than once to each rater. Since not all identities had all six emotions, the number of images in each counterbalanced condition ranged from 179 to 193.</p>
</section>
<section id="emotion-intensity-ratings" class="level5" data-number="3.2.3.2">
<h5 data-number="3.2.3.2" class="anchored" data-anchor-id="emotion-intensity-ratings"><span class="header-section-number">3.2.3.2</span> Emotion Intensity Ratings</h5>
<p>We obtained ratings of how intensely faces expressed each intended emotion (‘How intensely is this person expressing anger [disgust, fear, happiness, sadness, surprise]?’) from 1 (not at all) to 7 (very). Here, raters only rated all faces showing one emotion expression (e.g., all angry faces) and rated the intensity only of the intended expression (e.g., angry faces only rated on anger intensity).</p>
</section>
</section>
<section id="procedure-1" class="level4" data-number="3.2.4">
<h4 data-number="3.2.4" class="anchored" data-anchor-id="procedure-1"><span class="header-section-number">3.2.4</span> Procedure</h4>
<p>We collected ratings via Experimentum (DeBruine et al., 2020); structure files for the exact experimental setup are available at REPO. After a brief introduction to the study and online informed consent, each participant (rater) was randomly allocated to one of the ratings (e.g., rating all 205 standardised neutral faces on how memorable they look). All available faces were displayed one at a time, in a randomised order for each rater. The question/prompt and response scale remained visible at the top of the screen, above the photo throughout the study. The study automatically progressed to the next trial once the rater responded by clicking on the response scale. There was no time limit to provide a response. We included seven attention checks embedded in the study, which directed raters to provide a specific response. Following rating or categorising all stimuli, raters self-reported their gender, age, race/ethnicity, country of residence, and device type used for the study (desktop, laptop, tablet, mobile). They also completed an honesty/attention check question, asking them if they engaged with the study seriously, with assurance of payment regardless of response (choosing from ‘no, I was not really paying attention’ and ‘yes, I tried to give my authentic first impressions’). Raters were recruited through Prolific Academic. We collected all data in May 2025.</p>
</section>
<section id="participants" class="level4" data-number="3.2.5">
<h4 data-number="3.2.5" class="anchored" data-anchor-id="participants"><span class="header-section-number">3.2.5</span> Participants</h4>
<p>We aimed to collect 100 raters per rating condition (to achieve stable averages and allow for exclusions), totalling 2100 raters. Altogether, 2115 raters completed the study. See Results for exclusions and demographics of the final sample.</p>
</section>
</section>
<section id="results" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="results"><span class="header-section-number">3.3</span> Results</h3>
<section id="data-cleaning-and-exclusions" class="level4" data-number="3.3.1">
<h4 data-number="3.3.1" class="anchored" data-anchor-id="data-cleaning-and-exclusions"><span class="header-section-number">3.3.1</span> Data Cleaning and Exclusions</h4>
<section id="models" class="level5" data-number="3.3.1.1">
<h5 data-number="3.3.1.1" class="anchored" data-anchor-id="models"><span class="header-section-number">3.3.1.1</span> Models</h5>
<p>For data cleaning, we first downloaded and reshaped the raw data from Experimentum. In the next step, we ensured that the models’ gender, race/ethnicity, and units of height and weight were consistently formatted across labs. For gender and race/ethnicity, words presented in languages other than English were recoded to be presented in English (e.g., “mulher” to “female”, “preta” to “Black”). We then classified self-described race/ethnicity into one of seven categories (“White”, “Black”, “Asian”, “Indigenous”, “MENA” (Middle Eastern or North African), “Latine”, or “Mixed”), where possible. Descriptions that could not be clearly sorted into these categories were given “Ambiguous label” (non-entries were recoded as NA). For height, we ensured all data were presented in centimetres, and for weight, we ensured that all data were presented in kilograms. Models could report height and weight in metric or imperial units, so we converted from imperial to metric where required. We also sanity-checked the reported units, assuming heights &gt; 100 to be in centimetres and those &lt; 100 to be in inches, and weights &lt; 80 to be in kilograms. Any non-entries of height or weight for a model was recoded as NA.</p>
</section>
<section id="raters" class="level5" data-number="3.3.1.2">
<h5 data-number="3.3.1.2" class="anchored" data-anchor-id="raters"><span class="header-section-number">3.3.1.2</span> Raters</h5>
<p>In the raters’ demographic questionnaire, we standardized participants’ recording of their race/ethnicity similarly to the models’ by recoding their inputs into one of seven categories (“White”, “Black”, “Asian”, “Indigenous/Pacific Islander”, “MENA” (Middle Eastern or North African), “Latine”, or “Mixed”), or as “Ambiguous label” when this was not possible. Any non-entries were recoded as NA.</p>
</section>
<section id="data-exclusions" class="level5" data-number="3.3.1.3">
<h5 data-number="3.3.1.3" class="anchored" data-anchor-id="data-exclusions"><span class="header-section-number">3.3.1.3</span> Data Exclusions</h5>
<p>A total of 2115 raters completed 2158 rating or categorisation tasks. We found that some raters did not complete all trials in tasks, some raters completed more than one task, and some raters completed more than the maximum number of trials in a task (likely by restarting the study or bypassing the back button block). Therefore, we removed incomplete tasks from our data, retained raters’ first complete tasks, and filtered our duplicate trials by keeping only raters’ first ratings for a duplicated trial. After these exclusions, and before implementing the pre-registered plan for data exclusions, we had complete and clean data from 1936 raters.</p>
<p>Our pre-registered plan for data exclusions included removing raters who gave overly consistent responses, committed overly fast responses, self-reported not taking the study seriously when asked whether or not they completed the study authentically, and failed attention checks. In total, we excluded 249 raters for our pre-registered reasons for data exclusions. <a href="#tbl-exclusions" class="quarto-xref">Table&nbsp;3</a> shows the number of raters we excluded for each of our reasons.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [8]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [9]:</pre></div><div><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: Multiple raters met more than one exclusion criterion; therefore, the counts for individual criteria do not sum to the total number of raters excluded.</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">a =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-exclusions" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-exclusions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: The number of raters excluded from analysis.
</figcaption>
<div aria-describedby="tbl-exclusions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["a"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<p>We defined overly consistent responses as those raters who responded to at least 90% of the trials identically. We defined overly fast responses as those raters whose median reaction time fell below the 1st percentile of the overall distribution of median reaction times (see Figure 1 for the distribution of median reaction times). For our attention checks, the threshold for inclusion was to accurately complete six or more attention checks (i.e., participants were excluded if they failed one or more of the checks). After these exclusions, we had 1899 raters in our sample (see <a href="#tbl-raters-per-task" class="quarto-xref">Table&nbsp;4</a> for the number of raters per task).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [10]:</pre></div><div id="cell-fig-rt-distribution" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-rt-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rt-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-rt-distribution-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Median reaction time distribution"><img src="index_files/figure-html/fig-rt-distribution-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rt-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Median reaction time distribution
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [11]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [12]:</pre></div><div><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">a =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-raters-per-task" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-raters-per-task-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Number of raters per task
</figcaption>
<div aria-describedby="tbl-raters-per-task-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["a"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
</section>
<section id="rater-demographics" class="level5" data-number="3.3.1.4">
<h5 data-number="3.3.1.4" class="anchored" data-anchor-id="rater-demographics"><span class="header-section-number">3.3.1.4</span> Rater Demographics</h5>
<p>We collected the following demographic data from our raters (N = 1676 of 1899 provided data): Age, gender, residence, ethnicity, and devices on which the ratings were completed. See <a href="#fig-gender-age" class="quarto-xref">Figure&nbsp;4</a> and Tables 3-5 for demographic information.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [13]:</pre></div><div id="cell-fig-gender-age" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-gender-age" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gender-age-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-gender-age-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Histogram of rater age, separated by gender"><img src="index_files/figure-html/fig-gender-age-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gender-age-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Histogram of rater age, separated by gender
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [14]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [15]:</pre></div><div><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-raters-country" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-raters-country-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: Rater country of residence
</figcaption>
<div aria-describedby="tbl-raters-country-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [16]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [17]:</pre></div><div><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-raters-eth" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-raters-eth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: Rater race/ethnicity
</figcaption>
<div aria-describedby="tbl-raters-eth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [18]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [19]:</pre></div><div><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-raters-dev" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-raters-dev-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7: Devices used by raters to complete the study
</figcaption>
<div aria-describedby="tbl-raters-dev-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
</section>
</section>
</section>
<section id="agreement-indicators-for-the-ratings" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="agreement-indicators-for-the-ratings"><span class="header-section-number">3.4</span> Agreement Indicators for the Ratings</h3>
<section id="standardized-trait-ratings" class="level4" data-number="3.4.1">
<h4 data-number="3.4.1" class="anchored" data-anchor-id="standardized-trait-ratings"><span class="header-section-number">3.4.1</span> Standardized Trait Ratings</h4>
<p>In the first step, we calculated intraclass correlations for traits observed across tasks for our standardized images. The number of raters ranged from 83 to 94 for these traits. The average reliability across raters was very good for rating attractiveness, dominance, trustworthiness, and gender-typicality, but poorer for rating memorability (see <a href="#tbl-std-neu-agree" class="quarto-xref">Table&nbsp;8</a>).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [20]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [21]:</pre></div><div><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: ICCs values are ICC (2,k) values.</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-std-neu-agree" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-std-neu-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8: Agreement for ratings of standardized neutral faces
</figcaption>
<div aria-describedby="tbl-std-neu-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<p>Next, we examined the number of raters required for trait ratings to reach stable levels of reliability defined as ICC (2,k) values of 0.75 and 0.90 (see Figure 3). Attractiveness ratings stabilized with the fewest participants, reaching an ICC (2,k) of 0.75 at approximately 18 raters. It was also the only trait to exceed an ICC (2,k) of 0.90, which occurred with 50 raters. Ratings for dominance, trustworthiness, and gender-typicality reached an ICC (2,k) of 0.75 with approximately 35-40 raters.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [22]:</pre></div><div id="cell-fig-icc-cos" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-icc-cos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-icc-cos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-icc-cos-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Corridors of stability for ICCs"><img src="index_files/figure-html/fig-icc-cos-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-icc-cos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Corridors of stability for ICCs
</figcaption>
</figure>
</div>
</div>
</div></div>
<p>For checking internal consistency of our ratings, we used both Cronbach’s alpha (α) and McDonald’s omega total (ωt) (see <a href="#tbl-std-neu-agree" class="quarto-xref">Table&nbsp;8</a>). For four of the five traits, both α and ωt exceeded 0.9, indicating excellent internal consistency. For the fifth trait (memorability), α was lower (0.65), indicating doubtful internal consistency when assuming equal item contributions (tau-equivalence) in the ratings. However, ωt for memorability was substantially higher (0.87), indicating good internal consistency after taking into account variance among item or rater contributions.</p>
<section id="unstandardized-trait-ratings" class="level5" data-number="3.4.1.1">
<h5 data-number="3.4.1.1" class="anchored" data-anchor-id="unstandardized-trait-ratings"><span class="header-section-number">3.4.1.1</span> Unstandardized Trait Ratings</h5>
<p>As with our standardized trait ratings, but for three rather than five traits, we calculated intraclass correlations for ratings to our unstandardized images observed across rating tasks. The number of raters ranged from 84 to 95 for these traits. The average reliability across raters was very good for all trait ratings (see <a href="#tbl-unstd-neu-agree" class="quarto-xref">Table&nbsp;9</a>). All three traits also demonstrated excellent internal consistency (see <a href="#tbl-unstd-neu-agree" class="quarto-xref">Table&nbsp;9</a>) with Cronbach’s alpha (α) ranging from 0.88 to 0.95 and McDonald’s omega total (ωt) ranging from 0.90 to 0.96.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [23]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [24]:</pre></div><div><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: ICCs values are ICC (2,k) values.</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-unstd-neu-agree" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-unstd-neu-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9: Agreement for ratings of unstandardized neutral faces
</figcaption>
<div aria-describedby="tbl-unstd-neu-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
</section>
<section id="emotion-intensity-ratings-1" class="level5" data-number="3.4.1.2">
<h5 data-number="3.4.1.2" class="anchored" data-anchor-id="emotion-intensity-ratings-1"><span class="header-section-number">3.4.1.2</span> Emotion Intensity Ratings</h5>
<p>Next, we calculated the intraclass correlations and internal consistency metrics for our emotion intensity ratings. The number of raters ranged from 84 to 103 for our six emotions. The average reliability across raters was excellent for all of the emotions, with ICC (2,k) ranging from 0.95 to 0.98. Internal consistency was also excellent for all emotions in terms of Cronbach’s alpha (α = 0.97 to 0.99) and McDonald’s omega total (ωt = 0.97 to 0.99; see <a href="#tbl-emo-agree" class="quarto-xref">Table&nbsp;10</a>).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [25]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [26]:</pre></div><div><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: ICCs values are ICC (2,k) values.</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-emo-agree" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-emo-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10: Agreement for ratings of emotional faces’ emotion intensity
</figcaption>
<div aria-describedby="tbl-emo-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
</section>
</section>
<section id="points-of-stability" class="level4" data-number="3.4.2">
<h4 data-number="3.4.2" class="anchored" data-anchor-id="points-of-stability"><span class="header-section-number">3.4.2</span> Points of Stability</h4>
<p>To determine the number of raters required for stable ratings, we computed the point of stability (Hehman et al., 2018). This approach estimates the smallest sample size at which mean ratings stabilize by exceeding a cosine similarity threshold of 0.5 with the full-sample mean. As shown in Figures 4-6, the ratings to the standardized images reached stability between 37 and 45 raters, the ratings to the unstandardized images reached stability between 37 and 42 raters, and the ratings for the emotion intensities reached stability between 29 and 50 raters.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [27]:</pre></div><div id="cell-fig-std-neu-pos" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-std-neu-pos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-std-neu-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-std-neu-pos-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Points of stability for ratings of standardised neutral faces"><img src="index_files/figure-html/fig-std-neu-pos-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-std-neu-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Points of stability for ratings of standardised neutral faces
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [28]:</pre></div><div id="cell-fig-unstd-neu-pos" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-unstd-neu-pos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unstd-neu-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-unstd-neu-pos-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Points of stability for ratings of unstandardised neutral faces"><img src="index_files/figure-html/fig-unstd-neu-pos-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unstd-neu-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Points of stability for ratings of unstandardised neutral faces
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [29]:</pre></div><div id="cell-fig-emo-pos" class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-emo-pos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-emo-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-emo-pos-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Points of stability for emotion intensity ratings of standardised emotional faces of each expression"><img src="index_files/figure-html/fig-emo-pos-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-emo-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Points of stability for emotion intensity ratings of standardised emotional faces of each expression
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
</section>
<section id="descriptive-statistics" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="descriptive-statistics"><span class="header-section-number">3.5</span> Descriptive Statistics</h3>
<p>In this section, we report the descriptive statistics for ratings of the neutral standardized images, the neutral unstandardized images, the emotion categorization task, the intensity of expressed emotions, and the ages of the models.</p>
<section id="standardized-neutral-trait-ratings" class="level4" data-number="3.5.1">
<h4 data-number="3.5.1" class="anchored" data-anchor-id="standardized-neutral-trait-ratings"><span class="header-section-number">3.5.1</span> Standardized Neutral Trait Ratings</h4>
<p>Trustworthiness, dominance, and memorability ratings were approximately normally distributed and showed similar patterns of central tendency and variance. In contrast, attractiveness ratings were somewhat right-skewed, reflected in a relatively lower mean, while gender-typicality ratings were left-skewed, with a relatively higher mean (see <a href="#tbl-std-neu-desc" class="quarto-xref">Table&nbsp;11</a> for descriptive statistics and <a href="#fig-std-neu-hist" class="quarto-xref">Figure&nbsp;9</a> for rating distributions).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [30]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [31]:</pre></div><div><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-std-neu-desc" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-std-neu-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;11: Descriptive statistics for ratings of standardized neutral faces
</figcaption>
<div aria-describedby="tbl-std-neu-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [32]:</pre></div><div id="cell-fig-std-neu-hist" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-std-neu-hist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-std-neu-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-std-neu-hist-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Histograms for ratings of standardized neutral faces"><img src="index_files/figure-html/fig-std-neu-hist-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-std-neu-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Histograms for ratings of standardized neutral faces
</figcaption>
</figure>
</div>
</div>
</div></div>
<p>A key aim of this study was to provide normative data for each face model in the database. To visualize rating patterns, we created heatmaps with rating values on the x-axis and face models (separated by contributing lab site) on the y-axis. These heatmaps illustrate how the distribution of ratings varied across models and traits, with lighter tiles indicating that more raters gave a particular rating to that model (see Appendix). —capturing both between-model differences within a trait and within-model differences across traits</p>
<section id="unstandardized-neutral-trait-ratings" class="level5" data-number="3.5.1.1">
<h5 data-number="3.5.1.1" class="anchored" data-anchor-id="unstandardized-neutral-trait-ratings"><span class="header-section-number">3.5.1.1</span> Unstandardized Neutral Trait Ratings</h5>
<p>The ratings for dominance were approximately normally distributed while the ratings for attractiveness were slightly skewed right and the ratings for trustworthiness were slightly skewed left, each reflected in their relatively low and high mean ratings (see <a href="#tbl-unstd-neu-desc" class="quarto-xref">Table&nbsp;12</a> for descriptive statistics and <a href="#fig-unstd-neu-hist" class="quarto-xref">Figure&nbsp;10</a> for rating distributions).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [33]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [34]:</pre></div><div><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-unstd-neu-desc" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-unstd-neu-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;12: Descriptive statistics for ratings of unstandardized neutral faces
</figcaption>
<div aria-describedby="tbl-unstd-neu-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [35]:</pre></div><div id="cell-fig-unstd-neu-hist" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-unstd-neu-hist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unstd-neu-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-unstd-neu-hist-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Histograms for ratings of unstandardized neutral faces"><img src="index_files/figure-html/fig-unstd-neu-hist-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unstd-neu-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Histograms for ratings of unstandardized neutral faces
</figcaption>
</figure>
</div>
</div>
</div></div>
<p>As with the ratings of the standardized images, we produced heatmaps for each of the three traits capturing between-model differences within a trait and within-model differences across the traits (see Figure 10).</p>
<p>Emotion Categorisation and Intensity Ratings. Next, we explored raters’ emotion categorizations and perceived emotion intensity expressed by our models. Raters’ categorisations generally aligned with the models’ expression of emotion, with the greatest alignment was observed for expression of happiness, and the least for fear (<a href="#tbl-emo-desc" class="quarto-xref">Table&nbsp;13</a>, <a href="#fig-emo-freq" class="quarto-xref">Figure&nbsp;11</a>).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [36]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [37]:</pre></div><div><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: Note. Italicised values indicate correct categorisations (categorisations matching intended model expression).</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-emo-desc" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-emo-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;13: Emotion categorisation proportions
</figcaption>
<div aria-describedby="tbl-emo-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [38]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [39]:</pre></div><div class=""><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="fig-emo-freq" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-emo-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<div class="cell-output-display">
<div id="fig-emo-freq" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-emo-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-emo-freq-1.png" class="lightbox" data-gallery="fig-emo-freq" title="Figure&nbsp;11&nbsp;(a): Downward arrows indicate correct categorisations (categorisations matching intended model expression). A=anger, D=disgust, F=fear, H=happiness, S=sadness, U=surprise, O=other"><img src="index_files/figure-html/fig-emo-freq-1.png" class="img-fluid figure-img" data-ref-parent="fig-emo-freq" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-emo-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Downward arrows indicate correct categorisations (categorisations matching intended model expression). A=anger, D=disgust, F=fear, H=happiness, S=sadness, U=surprise, O=other
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-emo-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Emotion categorisation frequencies
</figcaption>
</figure>
</div></div></div>
</div></div>
<p>Next, we calculated the average ratings of emotion intensity for each emotion (<a href="#tbl-intensity-desc" class="quarto-xref">Table&nbsp;14</a>). These ratings were very similar, on average, between the different emotions except for happiness, which was rated with greater intensity than all other emotions. On average, these ratings were located around the middle of the 7-point scale. <a href="#fig-intensity-hist" class="quarto-xref">Figure&nbsp;12</a> shows the distribution of ratings on the scale for each emotion.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [40]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [41]:</pre></div><div><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-intensity-desc" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-intensity-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14: Descriptive statistics for emotion intensity ratings
</figcaption>
<div aria-describedby="tbl-intensity-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [42]:</pre></div><div id="cell-fig-intensity-hist" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-intensity-hist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intensity-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-intensity-hist-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Histograms of emotion intensity ratings"><img src="index_files/figure-html/fig-intensity-hist-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intensity-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Histograms of emotion intensity ratings
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
<section id="perceived-age" class="level5" data-number="3.5.1.2">
<h5 data-number="3.5.1.2" class="anchored" data-anchor-id="perceived-age"><span class="header-section-number">3.5.1.2</span> Perceived Age</h5>
<p>The distribution of raters’ perceptions of models’ ages is shown in Figure 13. 26-30 years was the most-chosen age across raters and models. The correlation between models’ mode perceived age and actual age appears in Figure 14, and heatmaps of perceived age for each model appear in Figure 15.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [43]:</pre></div><div id="cell-fig-age-hist" class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-age-hist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-age-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-age-hist-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: Histogram of perceived age"><img src="index_files/figure-html/fig-age-hist-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-age-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Histogram of perceived age
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [44]:</pre></div><div id="cell-fig-age-corr" class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-age-corr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-age-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-age-corr-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;14: Correlation between models’ actual age and mode perceived age"><img src="index_files/figure-html/fig-age-corr-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-age-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Correlation between models’ actual age and mode perceived age
</figcaption>
</figure>
</div>
</div>
</div></div>
<p>[link to where the data can be accessed – this can go on OSF, yes? Check Glasgow ethics]</p>
</section>
</section>
</section>
</section>
<section id="discussion" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="discussion"><span class="header-section-number">4</span> Discussion</h2>
<p>The value of the database</p>
<p>Provides proof of concept that various labs, all following the same protocol and using the same equipment, can take comparable images to form a coherent face database.</p>
<p>Ratings data provide information about the minimum number of raters needed for reliable mean ratings for different judgements</p>
<p>Reflections on the process (including feedback survey responses + our own reflections on this project)</p>
<p>Explaining certain methods choices &amp; what others might want to do in the future (e.g., in image processing, alignment, image size and face location driven by cropping needs - fix in future)</p>
<p>The problem of emotion - the emotion validation suggests that people are bad at either making or perceiving emotions - this is a very difficult thing and might be beyond the capacity of a project like this to gather good emotion images</p>
</section>
     </main>
<!-- /main column -->  <script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>  </div> <!-- /content -->  <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script> 
  
</body></html>